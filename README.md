# SKiPOD
CS MSU course


# Постановка задачи
•	Реализовать параллельную версию алгоритма RedBlack2D с использованием технологий OpenMP и MPI.

•	Исследовать масштабируемость полученной параллельной программы: построить графики зависимости времени исполнения от числа ядер/процессоров для различного объёма входных данных.

•	Cравнить эффективность работы параллельной реализации программы с использованием OpenMP и MPI. 

# Оптимизация

### Предоставленный алгоритм:

Было замечено, что исходная последовательная программа написана неэффективно. Были внесены следующие изменения, которые также повлияли на время работы:

•	Неэффективный проход по двойному массиву. Известно, что в языке Си массивы располагаются в памяти по строкам (сначала идут элементы первой строки, затем элементы второй строки и т.д.). Для последовательного доступа к памяти циклы были поменяны местами, что привело к ускорению.

•	Лишнее условие при инициализации массива A. Случаи, описанные в условиях, можно не рассматривать, так как глобальные массивы инициализируются нулями при создании.

•	В функции relax можно идти сразу по нужным нам элементам. Так мы обойдемся без лишнего условия, заодно снизим количество итераций.

### Распараллеливание 

#### OpenMP

Программа была распаралелена класссичиским средствами, предоставляемыми библиотекой OpenMP:

•	Наибольший выигрыш в производительности дает распараллеливание relax, так как эта функция вызывается наибольшее количество раз. Были распаралелены оба внешних цикла. В первом цикле есть разделяемая переменная eps. Эксперементы показали, что при использовании критической секции, время работы росло с увелечинением количества нитей, то есть данный подход неэффективен. Было решено использовать прагму reduction (max:eps).

•	Также был распараллелены функции init и verify.

#### MPI
•	Матрица разбивалась на равные блоки по строкам, каждый процесс получал один блок.

•	Взаимодействие соседних блоков (обмен граничных строк) было реализовано с использованием MPI_Irecv и MPI_Isend.

•	Для подсчета eps при каждом вызове функции relax() использовалась MPI_Reduce. 

•	Все блоки, распределенные между процессами, в конце собирались в итоговую матрицу при помощи операции MPI_Gather. 


# Результаты 

Работа задачи рассмотрена на суперкомпьютере Polus с различным числом нитей(1...160) для OpenMP/различном числе ядер(1...64) для MPI и различными размерами матрицы (514...8194). Каждое измерение проводилось 3 раза. Явные выбросы не учитывались, брался средний результат среди оставшихся.

### Таблица OpenMP 
![2dOpenMP](https://user-images.githubusercontent.com/56963957/102071642-d42cbd00-3e11-11eb-8c40-55706d54dfb6.png)

### Таблица MPI 
![2dMPI](https://user-images.githubusercontent.com/56963957/102071640-d42cbd00-3e11-11eb-9787-ccb782f24ec3.png)
  
### 3D график OpenMP 

![3dOpenMP](https://user-images.githubusercontent.com/56963957/102071637-d3942680-3e11-11eb-8bc4-cd9c7720c325.png)

### 3D график MPI 

![3dMPI](https://user-images.githubusercontent.com/56963957/102071634-d2fb9000-3e11-11eb-9e7a-45e3dee5deba.png)

### Сравнение OpenMP и MPI при N = 8194 

![OpenMP vs MPI ](https://user-images.githubusercontent.com/56963957/102071629-d131cc80-3e11-11eb-978e-3ec690630750.png)

 
 
 

# Анализ результатов

### OpenMP

Анализируя полученные замеры, можно заметить, что при количестве ядер 1, 2, 4, 8 можем наблюдать почти линейную зависимость от количества ядер. Дальнейшее распараллеливание постепенно приводит на плато, а в дальнейшем и к ухудшению показателей. Это происходит из-за роста накладных расходов. 

### MPI

Было установлено, что время работы алгоритма с использованием MPI стабильно уменьшается с увеличением количества процессов, т.е. для матриц любого размера выгоднее брать наибольшее количество процессов.


Эксперименты также показали, что OpenMP работает лучше при небольших размерах матрицы и небольшом количестве процессов/нитей, но разница во времени снижается с увеличением числа нитей/процессов. При больших размерах данных и большом количестве процессов/нитей MPI программа работает быстрее.


